

# This script is supplementary to the pipeline and allows one to check
# for consistency in loci alignment across multiple ids_v_cov.tsv files.
#
# Arguments:
# -i: input file list of ids_v_cov.tsv paths
# -min: minimum %ID threshold to collect by, so if 70 is input here then 
# only those entries that are at or above this threshold will be included
# as being part of the final file. 
# -max: same logic as the 'min', except the upper limit
# -p: prefix for the reference assembly loci that are trying to be 
# re-assembled. For loci formatted like PF3D7_0001 you would enter "PF3D7 here".
# -o: prefix for path to output TSV files for this script. It will output three
# files...
# 1. overall_out.tsv - contains a complete map of all loci to how many times they
# were assembled within the threshold range specified.
# 2. shared_out.tsv - contains counts for which files shared assemblies and how many
# times this given event occurred.
# 3. unique_out.tsv - map for loci and their input file they were uniquely assembled
# from. 
#
# Run the script using a command like this:
# python generate_scatterplot.py -i list_of_ids_v_cov_files.txt -m 70 -p PF3D7 -o output.tsv
#
# Author: James Matsumura

import argparse,re
from collections import defaultdict

def main():

    parser = argparse.ArgumentParser(description='Script to assess the consistency across ids_v_cov.tsv files generated by assess_alignment.py.')
    parser.add_argument('-i', type=str, required=True, help='Input file list of paths for ids_v_coverage.tsv.')
    parser.add_argument('-min', type=float, required=True, help='Minimum percent ID threshold to decide whether to include an entry.')
    parser.add_argument('-max', type=float, required=True, help='Maximum percent ID threshold to decide whether to include an entry.')
    parser.add_argument('-p', type=str, required=True, help='Prefix for all loci, so for PF3D7_0001 you enter "PF3D7".')
    parser.add_argument('-o', type=str, required=True, help='Prefix path to outfiles.')
    args = parser.parse_args()

    overall_dict = {} # dict to carry all the loci and their counts for how many times assembled
    shared_dict = {} # dict to hold loci that are shared across assembly methods
    unique_dict = {} # dict to hold loci that are unique to one assembly method

    # Assign column 1 of file to x, and column 2 to y
    with open(args.i,'r') as infile:
        for line in infile:
            ivc_file = line.rstrip()

            with open(ivc_file,'r') as file:
                simple_fname = re.search(r'/?([a-zA-Z0-9_\.]+)$',ivc_file).group(1)
                for line in file:
                    line = line.rstrip()
                    elements = line.split('\t')

                    id = float(elements[0])

                    if args.max >= id >= args.min: # if we've passed the minimum %ID, add to output
                        path = elements[2]
                        extraction_pattern = re.compile(r'/([A-Za-z0-9_\.]+)')

                        for captured in re.findall(extraction_pattern,path):
                            # bit of extra precaution to make sure this is indeed grabbing
                            # a locus which has the prefix set by the user and ends in some
                            # numerical value (so that we are not grabbing .txt files or
                            # something similar if the user used an odd path)
                            if captured.startswith(args.p) and re.search(r'\d+$',captured):
                                if captured in overall_dict:
                                    overall_dict[captured] += 1

                                    # If we've already taken care of the unique dict and 
                                    # added to shared dict before, go here
                                    if captured in shared_dict:
                                        shared_dict[captured] += " {0}".format(simple_fname)
                                    else: # need to remove entry from unique_dict
                                        both_files = "{0} {1}".format(unique_dict[captured],simple_fname)
                                        shared_dict[captured] = both_files
                                        unique_dict.pop(captured,None)
                                else:
                                    overall_dict[captured] = 1
                                    unique_dict[captured] = simple_fname

    # Write all outfiles
    write_outfile("{0}/overall_out.tsv".format(args.o),overall_dict)
    write_outfile("{0}/shared_out.tsv".format(args.o),shared_dict)
    write_outfile("{0}/unique_out.tsv".format(args.o),unique_dict)

# Arguments:
# outfile - path to outfile
# in_dict - dictionary to iterate over and write out
def write_outfile(out,in_dict):
    with open(out,'w') as outfile:
        for key in sorted(in_dict,key=in_dict.get,reverse=True):
            outfile.write("{0}\t{1}\n".format(key,in_dict[key]))


if __name__ == '__main__':
    main()
